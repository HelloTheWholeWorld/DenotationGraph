I. Generating Rewrite Rules and Starting Strings

To generate a denotation graph, you will first need to run:

scripts/runPre.sh NAME
Runs the scripts that generate a set of rewrite rules and a string for
each caption, that will be used to generate the denotation graph.  The
resultsing file will be "../corpora/NAME/graph/initial.txt" (see
"../corpora/notes.txt" for a description of its contents).

Afterwards to generate a new denotation graph, you will need to run:

scripts/runNew.sh NAME
Generates a denotation graph using "../corpora/NAME/graph/initial.txt"

Alternatively, you can extend a previously existing denotation graph:

scripts/runAdd.sh NAME OLD

Extends the denotation graph in "../corpora/OLD/graph" using
"../corpora/NAME/graph/initial.txt".  Use this if you want to use the
statistics (for example, co-occurrence, PMI, and conditional
probabilities) generated for the old graph with the new corpus.  Any
nodes in the old denotation graph that can be produced by the new
corpus should be produced, and the indicies will be the same.  This
should allow you to take advantage of anything generated using the old
graph.


II. Data Files
This section details the files in the "data" directory.  They are used
during rule generation to identify classes of words.

data/ccGroup.txt
Contains lists of entity mentions that can be grouped together by a
conjunction (CC).  This is intended to capture constructions such as
"hat and scarf" or "man or woman".  When one entity mention in such a
construction is dropped, the entire construction should be dropped.
Ideally, these constructions would be chunked so that the rewrite rule
scripts would not have to manually identify them.  However, since they
are not, "data/ccGroup.txt" assists them in identifying conjoined
entity mentions.

Each line contains a head noun.  Classes of head nouns are separated
by a blank line.  For example:

   apron	(start of group 1 - clothing)
   backpack
   ...
   shoulder
   tattoo
			(end of group 1)
   bottle	(start of group 2 - containers)
   cup
   ...

Only head nouns of the same class can be joined together by a
conjunction, so this will not recognize a construction such as "man
and lake", where "man" is part of the people class, and "lake" is part
of the landscape class.  Entity mentions that are of the form "X of Y"
can be represented as "X/Y", so the head noun of "body of water" would
be written "body/water".

This data file is used by "scripts/dropPPs.pl" and
"scripts/dropWearDress.pl".  Any rewrite rule script that can
potentially drop entity mention chunks should be using this file.

data/count.txt
Contains the normalizations of the various NP determiners that exist
in the corpus.  Each line contains two tab separated columns: the
original determiner and the normalized form.  So far, the normalized
forms are "one", "two", "three", and "some" for any determiner that is
more than three or is plural, but of indeterminate number.
"scripts/lemmaNouns.pl" uses this data file as part of the entity
mention/noun normalization process.

data/entmod.txt
This file contains a list of noun modifiers that may mean different
things when applied to a more generic head noun.  For example, "a
large mouse" is not necessarily "a large animal", even though "a
mouse" is "an animal".  So far, this list contains size and age noun
modifiers.  This file is used by "scripts/dropEntityMods.pl" to
determine if a rewrite rule that restores a noun modifier needs to
make sure that the head noun has not been changed.

data/ent-mod.txt
This is the default entity modifier chunking file.  Each line consists
of a head noun and a series of noun modifiers that can be chunked.
For example, the following line:

  ad     large     calvin klein

Indicates that for the entity mention "large calvin klein ad", where
"ad" is the head noun and "large calvin klein" are the noun modifiers,
"large calvin klein" can be chunked as "large" and "calvin klein".
Additional noun modifier chunks are indicated by adding additional
columns, for example:

  area   busy   downtown    shopping

Indicates that for "busy downtown shopping area", each of "busy",
"downtown", and "shopping" should be chunked individually.  This file
is initially generated by "../entity/scripts/chunkEntityMods.pl".
However, that script has several limitations - it can never find more
than two noun modifier chunks for any entity mention, and it pays
attention to punctuation, whereas the "data/ent-mod.txt" file assumes
that all punctuation has been removed during normalization.  Make sure
you manually go over the results of
"../entity/scripts/chunkEntityMods.pl" before using the resulting
output.

Additionally, entity modifier chunking files can be made specifically
for a corpus, by creating a "../corpora/NAME/NAME.ent-mod" file.
"scripts/lemmaNouns.pl" uses this file to rechunk the noun modifiers.

data/lexicon.txt
A lexicon containing a set of more generic terms (hypernyms) for head
nouns.  A head noun can have multiple hypernyms, but it assumed that
each head noun will have a single unique most generic term.  For
example, "male" has both "man" and "person" as hypernyms.  "person" is
the most generic term for "male", because it is the most generic term
for both "man" and "person" (obviously, in the second case).  The
lexicon is used by "scripts/liftEntity.pl" to replace the head nouns
of entity mentions with their hypernyms.  The lexicon is generated by
"../entity/scripts/makeHypeLexicon.pl".  The script generates the
lexicon by trying to perform word sense disambiguation using the
entity coreference chains, and my be incorrect.  Furthermore, it will
be unable to identify any hypernyms that are not in WordNet.  Thus,
the output needs to be both manually checked over, to ensure that the
hypernyms are correct (watch out for words with multiple senses in the
corpus - the script will handle these incorrectly), and to augment the
lexicon with hypernyms that are not in WordNet.

data/light-verbs.txt
A list of of light verbs, or verbs where dropping the object will
change the meaning of the event.  Some of these are somewhat corpus
specific: "play baseball" -> "play" is technically correct, however in
the image caption corpus, "play" by itself really means "playing
around", rather than playing some type of game.  Used by
"scripts/splitSubjVerb.pl" to determine whether or not to generate a
rewrite rule to extract a transitive verb from the verb phrase it
occurs in.

data/person.txt
A list of head nouns that are person terms.  Used by
"scripts/lemmaNouns.pl" to extend those head nouns if they are
preceded by an age term.  For example, "son" preceded by "toddler"
would extend the head noun to "toddler son", instead of just "son".
Not completely sure what my intent was for this, at this point.

data/player.txt
A list of modifiers that can be applied to "player" that should be
part of the head noun.  Similar to above, in cases where the head noun
is "player" and the preceding modifier is one of the terms in
"data/player.txt", the head noun will be changed to include the
modifier.  So, "player" preceded by the modifier "accordion" will be
changed to "accordion player".  Again, this is similar to how "play"
is treated as a light verb - "player" by itself combines a large
number of types of playing that we do not want to conflate - hence, we
create new head nouns using the modifier and "player", where
appropriate.

data/stopwords.txt
A standard list of stopwords.  Primarily exists so that stopword only
strings/nodes can be ignored when doing image counts.  Used by
"scripts/countWords.pl" and "scripts/countWordsCap.pl".  For actual
graph generation, stopwords are either important for determining when
a particular rewrite rule should be applied (function words that
connect chunks), or already normalized.


III. Pre-generation Scripts
This section covers the scripts used by "scripts/runPre.sh".  The
first half of the scripts perform normalizations and lemmatizations.
The second half of the scripts generate the starting string and
rewrite rules.  A number of the scripts use functions defined in
"scripts/simple.pm" - see section V for a description of what they do.

scripts/assignWordIDs.pl
This script adds IDs to each token (including the chunk boundaries) in
a caption.  The IDs will be used by the rewrite rules to uniquely
identify tokens to add or remove.  The IDs will be the first element
of each token (with '/' being the separator.

   The/DT --> 2/The/DT

scripts/addIDs.pl
Adds "NP#" and "VP#" ids to the beginning on EN chunks and VP chunks.
These IDs will match those used by the entity co-reference system
("../entity") and the subject/verb/object triples ("../event").  The
ID will always be the second field of the beginning boundary of an EN
or VP chunk.  In the case of EN chunks, this displaces the
co-reference ID to the third field.

  1/[EN/10 --> 1/[EN/NP1/10

scripts/lemmaNouns.pl
Lemmatize nouns, replace determiners with "two", "three", or "some"
(singular determiners are just dropped at this point).  Chunk noun
phrase modifiers further, if possible.  Also, moves the last word of
the modifier into the head, if it is a recognized compound noun.
(I.e., "wooded area" is initially chunked with "wooded" as the
modifier and "area" as the head noun.  This will rechunk it as "wooded
area" as the head noun).  "data/player.txt" contains a list of types
of players that should be a single head noun (i.e., "accordion
player").  "data/person.txt" contains a list of person terms, which,
when combined with an age term are considered compound terms ("toddler
son", etc.).  Additionally, two more compound terms are hard coded
into the script ("wooded area" and "skate park").

scripts/lemmaVerbs.pl
Lemmatize verbs, and normalize VP chunks, by dropping auxiliary verbs,
dropping "to"s (but leaving in "X to Y" cases - those are handled by
"scripts/splitSubjVerb.pl").  Also, for some of the verb
lemmatizations, use the -ing form of the verb, if we don't want to
conflate the noun and verb forms ("park" vs. "parking", "stand"
vs. "standing", etc.).  For the verb "dress" used "dressed".

scripts/lowerCase.pl
Lower case all of the words.

scripts/dropPunct.pl
Drop punctuation marks.  In this case, any non-chunk boundary that has
a POS tag that does not start with a letter.

scripts/lemmaTeens.pl
Normalize any word that any word that starts with "teen-" ("teenaged",
"teenager", "teens"), to "teen".

scripts/typeGirl.pl
Change "girl" to either "girl_woman" or "girl_child", depending on the
other captions of the image.  (Basically, if they use a term that is
likely to refer to a woman.)

scripts/dropThereBe.pl
Drop initial "there/this/here be" from captions.

scripts/lemmaWear.pl
Normalize "dressed in" and "dressed up in" as "wear".  Second, collect
all the clothing head nouns, based on the EN chunk that comes after
any "wear" VP chunk.  Convert "in <clothing>", where "<clothing>" is
an EN chunk with a head noun that's a clothing noun (as determined
earlier) to "wear <clothing>".

scripts/tokenize.pl
Produces a file containing only the words in a caption - strips out
other metadata.  Not acutally used by anything else, but will let you
see what the result of all of the normalizations and lemmatizations
are.  File output (when run be "scripts/runPre.sh") is
"tmp/NAME/pre.token".

scripts/dropEventMods.pl
Drop ADVP chunks and adverbs (RB) inside VP chunks.  This can be
problematic if we have a VP chunk that consists of only adverbs -
we're not checking for that.

scripts/dropEntityMods.pl
Drop the insides of NPM/NPMC chunks.  "data/entmod.txt" contains a
list of entity modifiers that can only be restored if the original
head noun is in place.  They're all size/age modifiers - i.e., "a
large mouse" is not a "large animal".  Also, do not drop modifiers of
"hair", because "person with black hair" -> "person with hair" isn't a
transformation that we want.  (Note: possibly add that to the lemma
nouns part - have it move the modifier into the NPH chunk.)

scripts/dropEntityArticle.pl
Drops the insides of NPD chunks, unless the NPD chunk is "no" or
"each".

scripts/liftEntity.pl
Replaces NPH chunks with more generic terms, as detailed in
"data/lexicon.txt".  A couple terms with multiple meanings will not be
replaced unless they are subjects (and therefore probably people) -
"batter", "diner", "pitcher", "speaker".  Will also generate
additional rules to remove age terms from the beginning of head nouns,
although the rules might not actually be used.

scripts/splitXofY.pl
Generates two rules to turn an "X of Y" EN chunk into an "X" EN chunk
and a "Y" EN chunk.  First of the reduction rewrite rules.

scripts/splitXorY.pl
Generates two rules to turn an "[EN X ] or [EN Y ]" sequence into "[EN
X ]" and "[EN Y ]".  Another reduction rewrite rule.

scripts/dropPPs.pl
Removes prepositional phrases (along with additional parts of the
string to make the resulting string after removing the prepositional
phrase grammatically correct), and generates rules to restore the
prepositional phrase.  Uses "data/ccGroup.txt" to identify cases where
the prepositional object includes multiple entity mentions.

If the prepositional phrase is preceded by a chunk, the rewrite rule
will actually insert/attach the prepositional phrase to the chunk.
This allows the prepositional phrase to be restored in a larger set of
circumstances.

scripts/dropWearDress.pl
Drop instances of "wear X" and "dressed [in/as/for] X".  Also, "X" can
be "X CC Y", if "X" and "Y" are of the same type.  "data/ccGroup.txt"
lists head nouns that are of the same type.  Generates rules to
restore the dropped phrases.

scripts/dropTail.pl
Generates rules to drop trailing "and" or "while".  Note: these rules
are actually fixing up screw ups earlier in the generation process.

scripts/splitSubjVerb.pl
Generate the rules for extract SVOs from complex sentences, splitting
subject + VPs, verbs + direct objects.  Also, generate rules for
splitting up TOs in verbs (for example, "jump to catch Frisbee" ->
"jump" and "catch Frisbee").

scripts/emptyBrackets.pl
Detect empty brackets in the strings.  Give it a sequence of caption +
rewrite rule files.  It will find the first file in the sequence that
a given empty bracket occurs in.  Some of the empty brackets are
intentional - NPD, NPM, and NPMC chunks can all be emptied by
reductions and the brackets left as anchor points for the rewrite
rules.  On the other hand, VP and ADVP empty brackets may indicate
that something is wrong.  (At least one empty VP chunk occurs because
it only contains RBs that get removed by "scripts/dropEventMods.pl").


IV. Graph Generation Scripts
There are two graph generation shell scripts:
scripts/runNew.sh - Generate a new denotation graph from the given
corpus.

scripts/runAdd.sh - Generate extend an existing denotation graph using
the given corpus.  Extension means that the strings in the existing
denotation graph will be used in the new denotation graph.  This means
that the node indices of the existing denotation graph will still be
valid in the new denotation graph.  Also, it means that captions in
the new denotation graph will be associated with any strings from the
old denotation graph that they can generate.

The only difference between these two scripts are the calls to the
"scripts/makeOutput*.pl" scripts.  These scripts generate the
denotation graph and various sub-graphs - for "scripts/runAdd.sh", the
scripts are called with an additional argument, indicating the
previously existing denotation graph to extend.

"scripts/simple.pm" (see section V) contains the functions that apply
rewrite rules to strings.  The actual bookkeeping and decisions on
which rewrite rules to apply to which strings are handled by the
"script/makeOutput*.pl" scripts.

scripts/makeOutputNP.pl
Generate the NP sub-graph.  This consits of all the EN chunks that we
are pretty sure are correct (the full original version, and the fully
reduced versions), along with edges between them.

scripts/makeOutputVP.pl
Generate the VP sub-graph.  This consists of the VP parts of all SVO
triples we can find.  Additionally, we will use any modifier dropping
rule to generate other forms of the VPs.  Uses the NP sub-graph as a
starting point.

scripts/makeOutput.pl
Generates a denotation graph.  Uses the NP and VP sub-graphs.  The NP
sub-graph is used to identify entity mentions for SENT links, while
the VP sub-graphs guarantees that events will have a certain amount of
specificity in the graph.  (You can probably skip the VP sub-graph
generation, but there's no real reason to.)

scripts/makeOutputOrig.pl
Adds the original captions into the denotation graph.  The leaf nodes
of a caption have ORIG edges added to the original caption.
Additionally, the subject-verb extraction rewrite rules are run on the
original captions, and the appropriate edges are added.  (The VP and
NP sub-graphs should ensure that the extracted nodes are not orphans.)

scripts/propogateImage.pl
It may be the case that child nodes are produced by captions that
their parents are not.  While there are some valid reasons for this,
this script propogates a child node's captions to its parents.

scripts/makeSentImg.pl
Takes a node-caption map, and turns it into a node-image map.

scripts/flipMap.pl
Invert a map - take a node-caption map, and produce a caption-node
map.

scripts/combineChunk.pl
Combine multiple chunk files together.

scripts/typeChunk.pl
Assigns a type to each possible chunking of a node (VP, EN, SN).

scripts/makeSubgraph.pl
Generate sub-graphs from a denotation graph.  Subgraphs consist of
only those edges and nodes that are produced by a specific set of
images.  Takes a file consisting of two tab separated columns:
sub-graph name, and a file consisting of the images in the sub-graph.
Prints out the names of the sub-graphs generated.

script/subgraph.pl
Generates a subgraph of a given denotation graph given a list of
images to include in the subgraph.

scripts/countNodes.pl
Determine the size of the visual denotation of each node (ignoring
nodes below a given threshold), and the size of the intersection of
each pair of nodes' visual denotations (again, ignoring cases where
the intersection if below a given threshold).

scripts/calcPMI.pl
Use the image counts/visual denotation sizes and the co-occurrence
counts to calculate PMIs and conditional probabilities for each pair
of nodes with a co-occurrence count.


V. Other Scripts
scripts/getOrig.pl <caption ID> <graph directory>
Will produce the marked up versions of strings (including bracketing
and token IDs) produced by the given caption in the graph.  This
script will not generate the strings that are produced using implicit
rules (entity extractions from strings (SENT links), the original
caption (ORIG links), and some direct object extractions (TVERB
links)).



VI. simple.pm
"scripts/simple.pm" contains a number of common functions.  There are
four main groups:

"applyRule" and "generateSentences" handle the rule applications.
"applyRule" applies a single rule to a string, while
"generateSentences" generates all of the possible strings using a set
of rules and some conditions on how the rules may be applied.

"printSentence", "getMaxID", and "addTransformation" are used by the
rewrite rule generating scripts.  They handle some simple
functionality used in adding and printing out rewrite rules.

"plain", "chunk", and "ids" produce reduced forms of strings.
Normally a string includes a bunch of meta-data - token IDs, chunk
boundaries, co-reference chain IDs, POS tags, etc.  "plain" produces
the plain text version of a string, "chunk" produces a plain text +
chunk boundaries version of the string (making sure to strip out empty
chunks), and "ids" produces the sequence of token IDs that make up the
string.

"loadVPs", "countVPs", and "getVP" to get the SVO triples identified
by the "../event" scripts.
