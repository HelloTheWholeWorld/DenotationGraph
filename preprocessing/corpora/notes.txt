I. Caption and Chunk IDs

Almost every file in a corpus directory will use caption IDs.  Our
current format is:

<image ID (file name)> # <caption ID (index)>

Captions that describe the same image must have the same image ID
(portion of the caption ID to the left of the hash) - using the image
file name makes this happen automatically, as well as making the
actual image being captioned easier to locate.  The caption ID can be
any string that does not contain hashes or tabs.  We use indices
starting from 0, but there is no real restriction on what a caption ID
can be.

2825897091.jpg#1 is the second caption of image "2825897091.jpg".

A number of files will also reference particular chunks in a caption.
Chunk IDs are of the form:

<image ID> # <caption ID> # <chunk> <chunk #>

The first two parts are the caption ID, while <chunk> is the type of
chunk (VP for VP/verb phrase chunks, NP for EN/entity mention chunks).
The chunk number indicates which chunk in the caption is being
referenced, and starts from 0.

2825897091.jpg#1#NP2 is the third EN chunk of 2825897091.jpg#1.


II. Corpus Files

Corpus files will live in the "NAME" sub-directory, and be named
"NAME.TYPE", for the most part.  I will list the files in the order
they are created, describe their contents, and how to create them.


NAME.spell
Columns: caption ID, spelling corrected caption
Generated based on the spelling corrected results from CrowdFlower.
See "../crowdflower/notes.txt".


NAME.pretoken
Columns: caption ID, tokenized caption
Input: NAME.spell
Script: ../token/scripts/run.sh NAME
Tokenized version of "NAME.spell".  Generated using the OpenNLP
tokenizer.


NAME.token
Columns: caption ID, tokenized caption
Input: NAME.pretoken
Script: ../compound/scripts/run.sh NAME
"NAME.pretoken" with a couple spelling fixes, tokenizer fixes, and
compound term normalizations.


NAME.pos
Columns: caption ID, tagged and chunked caption
Input: NAME.token
Script: ../pos/scripts/run.sh NAME
Adds the tagging and chunking information generated by the OpenNLP
tagger and chunker, and applies a number of fixes.  See
"../pos/notes.txt" for a description of the fixes.


NAME.np
Columns: NP chunk ID, starting word, ending word, preceding
         preposition, coref ID, succeeding preposition, synset1,
         determiner1, modifier1, head1, [joining preposition,
         synset2, determiner2, modifier2, head2]
Input: NAME.pos
Script: ../entity/scripts/run.sh NAME
Contains all of the entity mention chunks in the corpus.  Provides a
bit of the context that the entity mention occurs in, as well as the
internal chunking of the entity mention.  It is not guaranteed that a
synset will actually be included - when it is, it either means, it's a
hard coded synset, it's required to be that type of synset in order
for an X of Y to work (i.e., "pitcher of water" - "pitcher" must be a
container, and therefore must be "pitcher, ewer" and not "pitcher,
hurler, twirler"), or there was only one synset possible for head
noun.  In the case of "X of Y" entity mentions, an additional five
columns include the preposition that joins X and Y (should always be
"of/IN"), and the synset, determiner, modifier, and head of Y.

(_ indicates a blank column, columns are separated with two spaces)

1001773457.jpg#0#NP1  4  6  _  89  with/IN  dog#n#1  a/DT  white/JJ  dog/NN

Which is from the caption:

A black dog and a white dog with brown spots are staring at each other in the street .

1001773457.jpg#0#NP1 spans tokens 4 through 6 ("a white dog"), has no
preposition directly proceeding it, is part of co-reference chain 89,
is directly succeeded by the preposition "with/IN", is of type dog#n#1
(dog, domestic dog, Canis familiaris), has "a/DT" as a determiner,
"white/JJ" as a modifier, and "dog/NN" as a head noun.

1001573224.jpg#4#NP0  0  5  _  69  _  class#n#1  A/DT  ballet/NN  class/NN  of/IN  girl#n#1  five/CD  _  girls/NNS

1001573224.jpg#4#NP0 spans tokens 0 through 5 in caption jpg#4, has no
directly preceeding or succeeding preposition, is part of co-reference
chain 69, is of the form "X of Y", where X has the synset class#n#1
(class, category, family), determiner "A/DT", modifier "ballet/NN",
and head noun "class/NN", and Y has the synset girl#n#1 (girl, miss,
missy, young lady, young woman, fille), determiner "five/CD", no
modifier, and head noun "girls/NNS".  Also, they are joined by the
preposition "of/IN".


NAME.coref
Columns: caption ID, tagged, chunked, and co-referenced caption
Input: NAME.pos
Script: ../entity/scripts/run.sh NAME
Contains entity mention (EN) chunks which contains NP chunks (note:
this the first time the chunks will be nested).  EN chunks contain
co-reference IDs, and NP chunks may contain synsets.  See
"../entity/notes.txt" for a description of the entity co-reference
resolution system.

Caption 1001573224.jpg#4:

[EN/69 [NP/class#n#1 A/DT ballet/NN class/NN ] [PP of/IN ] [NP/girl#n#1 five/CD girls/NNS ] ] [VP jumping/VBG ] [PP in/IN ] [EN/77 [NP sequence/NN ] ] ./.

There are two EN chunks - the first contains two NPs, while the second
contains one.  The first EN chunk is part of co-reference chain 69,
while the second is part of co-reference chain 77.  "A ballet class"
is assigned the synset class#n#1, while "five girls" is assigned the
sysnet girl#n#1.  "sequence" has no assigned synset.


NAME-malt-lin.conll
NAME-malt-poly.conll
NAME-malt-stanford.conll
Columns: Caption ID, token ID, token,
         token ID of dependency head, dependency type
Input: NAME.coref
Script: ../parser/scripts/run.sh NAME <# of processes>

Contains the parser outputs of the linear and polynomial models of the
MALT Parser and the Stanford Dependency Parser, in the CoNLL format.
There is one line per token, with different captions separated by a
blank line.  Token IDs will either be the index of the token (just a
number), or <chunk ID> # <index>.

1000092795.jpg#2        NP0#0   Two     NP0#1   num
1000092795.jpg#2        NP0#1   men     VP0#1   nsubj
1000092795.jpg#2        2       in      NP0#1   prep
1000092795.jpg#2        NP1#0   green   NP1#1   amod
1000092795.jpg#2        NP1#1   shirts  2       pobj
1000092795.jpg#2        VP0#0   are     VP0#1   aux
1000092795.jpg#2        VP0#1   standing        0       null
1000092795.jpg#2        7       in      VP0#1   prep
1000092795.jpg#2        NP2#0   a       NP2#1   det
1000092795.jpg#2        NP2#1   yard    7       pobj
1000092795.jpg#2        10      .       VP0#1   punct

NP0#0 is the first token of the first EN chunk, VP0#1 is the second
token of the first VP chunk, etc.  The intent is to make it easier to
identify which chunks are joined by a particular type of dependency.
For example, "nsubj" links NP0 and VP0, so the NP0 chunk is probably
the subject of VP0.


NAME.vp
Columns: verb chunk ID, verb
Input: NAME-malt-poly.conll
Script: ../event/scripts/run.sh NAME

Contains the lemmatized/normalized version of the verb.  Normalization
includes dropping terms that precede a TO, dropping auxillary verbs,
and non-verbs in the VP.  The VP:

[VP seem/VBP to/TO almost/RB obliverate/VB ]

is reduced to "obliverate".  "seem/VBP" is before a "to/TO", and is
dropped.  "to/TO" is also dropped, and "almost/RB" is not a verb, and
is dropped.


NAME.subj
Columns: verb chunk ID, subject chunk ID
Input: NAME-malt-poly.conll
Script: ../event/scripts/run.sh NAME
List of VP chunks and their associated subject EN chunk.


NAME.dobj
Columns: verb chunk ID, particle, object chunk ID
Input: NAME-malt-poly.conll
Script: ../event/scripts/run.sh NAME

List of VP chunks and their associated direct object EN chunk.  If a
particle occurs between the VP chunk and EN chunk, it will be
included.

1000523639.jpg#3#VP0  up  1000523639.jpg#3#NP2

1000523639.jpg#3#VP0 is "stitch", and 1000523539.jpg#3#NP2 is "coat".


NAME.svo
Columns: verb chunk ID, subject, verb, direct object
Input: NAME-malt-poly.conll
Script: ../event/scripts/run.sh NAME
List of VP chunks and their subject, verb, and direct objects.
Subject and/or direct object may be blank if no subject or direct
object was found.


NAME.split
Columns: sub-graph name, image list file
Used by: ../graph/scripts/run.sh, ../graph/scripts/makeSubgraph.pl
A configuration file to indicate which sub-graphs the denotation graph
generator should make.

train   results_20130124-train2.img
test    results_20130124-test2.img
dev     results_20130124-dev2.img

This generates three sub-graphs: train, test, and dev, using the
images in the .img files.


NAME.ent-mod (optional)
Columns: head noun, modifier chunk*
Used by: ../graph/scripts/run.sh
Initial generation: ../entity/scripts/chunkEntityMods.pl
This contains a list of noun phrases whose modifiers can be further
chunked.  Each line is a noun phrase, first column is the head noun of
the noun phrase, while the remaining columns are the individual
modifier chunks.

area   busy   downtown   shopping

The head noun is "area", the entire noun phrase "busy downtown
shopping area", and each of "busy", "downtown", and "shopping" will be
chunked independently.  This will allow the denotation graph to
generate "busy area", "downtown area", "shopping area", etc.  You can
use "../entity/scripts/chunkEntityMods.pl" to generate a starting
point for this file.  However, it has a number of limitations: it does
not pay attention to commas (and in fact will include them inside of
the chunks), and it can only identify two modfier chunks at a time,
the above line might be rendered as:

area   busy   downtown
area   busy   downtown shopping
area   downtown   shopping

Assuming the appropriate noun phrases exist in the corpus to identify
the various chunkings.  However, it will not be able to determine that
each of "busy", "downtown", and "shopping" can actually act as
independent chunks.  You will need to perform some manual edits to
create an approprate .ent-mod file.


NAME.nph-synset (optional)
Columns: head noun, synset
Used by: ../graph/scripts/buildLexicon.pl
Initial generation: ../entity/scripts/typeCorefs.pl
This contains a global synset assignment for all of the head nouns in
the corpus.  It will assign a single synset to each head noun, so a
noun such as bike only has the entry:

bike   bicycle#n#1

Which means that it only knows the bike sense of "bicycle, bike,
wheel, cycle" and not "motorcycle, bike".  Manually adding additional
bike entries should be okay, however.


NAME.lexicon (optional)
Columns: head noun, parent noun
Used by: ../graph/scripts/run.sh
Initial generation: ../graph/scripts/buildLexicon.pl
This contains the hypernym lexicon, a mapping of nouns to their more
generic parents.  A noun can have multiple entries in the hypernym
lexicon, however the graph generation process will assume that all
nouns have a single most generic ancestor.  For example, "male" might
have "man" and "person" as hypernyms, but "person" is an ancestor of
"man" as well, and has no ancestors, so "person" is the most generic
ancestor of "male".


NAME.lemma (optional)
Columns: caption ID, lemmatized caption
Initial generation: ../lemma/scripts/lemmatizer.pl
A lemmatized version of the corpus.  Lemmatization requires the part
of speech tags, and is done to individual tokens.  Compound nouns are
not treated as compound nouns.


NAME.nlemma (optional)
Columns: origional token, noun lemmatized token
Initial generation: ../lemma/scripts/lemmatizer.pl
A list of noun lemmatizations used when producing "NAME.lemma"


NAME.vlemma (optional)
Columns: origional token, verb lemmatized token
Initial generation: ../lemma/scripts/lemmatizer.pl
A list of verb lemmatizations used when producing "NAME.lemma"


NAME.cc (optional?)
Columns: image file name
A list of images that have been released under Creative Commons, or at
least that we should, in theory, be in the clear with regards to
re-distributing.



III. Denotation Graph Files

The denotation graph consists of a set of strings, a set of captions
that generates each string, and a set of edges linking strings that
can generate each other.  For example, we might have:

guard --(Xof)--> crowd of guard

"guard" is generated by captions: 103106960.jpg#1, 105077209.jpg#1,
105077209.jpg#3, 105077209.jpg#4, ..., 3974197857.jpg#4, ...

"crowd of guard" is generated by caption: 3974197857.jpg#4

Denotation graph files have fixed names.  In general, "np" will refer
to nodes in the NP sub-graph (a sub-graph consisting of entity
mentions), "vp" will refer to nodes in the VP sub-graph (a sub-graph
consisting of the verb-phrases, verbs, and direct objects), "node"
will refer to nodes in the fill graph, "cap" will refer to captions,
"img" will refer to images.  So, "node-cap.map" is a map from nodes in
the denotation graph to captions.


graph/img.lst
Columns: image file name
List of images used to generate this particular graph.  Assume that
all of the captions of the image were used.

graph/initial.rewrite
Columns: rewrite rule: #, dependencies, X, Y, rule type
         starting string: caption ID, next token ID, string
A list of rewrite rules and the starting string of each caption.  For
each caption, there will be a list of rewrite rules, followed by the
caption ID + starting string.

(-> are not part of the file format, but are included to make the X ->
Y columns more intuitive.)

0  _     15 17  ->  15 16/green/JJ 17                                                                                               +NPMOD/green
1  _       2 4  ->  2 3/two/DT 4                                                                                                    +NPART/two
2  _    5 36 7  ->  5 6/man/NNS 7                                                                                                   +NPHEAD/man
3  _    5 37 7  ->  5 36/adult/NN 7                                                                                                 +NPHEAD/adult
4  _  18 38 20  ->  18 19/shirt/NNS 20                                                                                              +NPHEAD/shirt
5  _     24 25  ->  24 26/[PP 27/in/IN 28/] 29/[EN/NP2/1 30/[NP 31/[NPH 32/yard/NN 33/] 34/] 35/] 25                                +PP/in/yard
6  _      9 23  ->  9 10/[VP/VPwear 11/wear/VB 12/] 13/[EN/NP1/2 14/[NP 15/[NPM 17/] 18/[NPH 38/clothing/NN 20/] 21/] 22/] 23       +WEAR/clothing
7  _  B 0 23 E  ->  B 0 E                                                                                                           -VERB
8  _  B 0 23 E  ->  B 23 E                                                                                                          -SUBJ
1000092795.jpg#2   26    0/[EN/NP0/0 1/[NP 2/[NPD 4/] 5/[NPH 37/person/NN 7/] 8/] 9/] 23/[VP/VP0 24/standing/VBG 25/]

The first nine lines are rewrite rules, while the last line is the
starting string.  The starting string is similar to the captions in
the .coref file, with a couple additions.  First, each token has been
assigned an ID (the first number of each token - "1/[NP" is token 1,
which is "[NP").  Second, "[EN" and "[VP" chunks have chunk IDs.
"0/[EN/NP0/0" means this is token 0, which is "[EN", which has the
chunk ID "NP0", and is part of co-reference chain 0.  Finally, NPD (NP
determiner), NPM (NP modifiers), NPH (NP head) chunks will show up in
the NP chunks.  Additionally, NPMC (NP modifier chunks) will show up
in NPM chunks, if we have determined that the noun phrase modifiers
can be dropped independently.

For the rewrite rules, dependencies are never used, and that column
should be always blank.  Rewrite rules either add something or drop
something, and this is indicated by the type.  For example, rewrite
rule 0 has a type "+NPMOD/green", which means it adds (+) the noun
phrase modifier (NPMOD) "green".  Rewrite rule 7 has the type "-VERB",
which means it drops (-) the verb phrase (VERB).  For the actual
graph, the strings generated by drop rules (-) are more generic,
while the strings generated by add rules (+) are more specific.  Edges 
are adjusted accordingly, and the add/drop symbol (+/-) is removed for 
the edge label.

Rewrite rules work by looking for the sequence of IDs on the left side
of the rule (X), and replacing it with the right side of the rule (Y).
For example, rewrite rule 1: 2 4 -> 2 3/two/DT 4, would look for the
tokens 2 4 in sequence (2/[NPD 4/] in the starting string), and then
insert "3/two/DT" between them, producing "2/[NPD 3/two/DT 4/]".  The
type of the rule os "+NPART/two", with means that something is being
added (+), and it's the NP article/determiner "two".  Consider rewrite
rule 7: B 0 23 E -> B 0 E.  This requires the sequence beginning of
string (B), chunk 0, chunk 23, and the end of string (E), and replaces
it with chunk 0 (the subject).  Thus the type -VERB, means the rule
drops (-) the verb phrase (VERB).

The token IDs that make up a rewrite rule can either refer to a token
or a chunk.  The 2 in rewrite rule 1 refers to the token "2/[NPD",
while the 0 in rewrite 7 can refer to any instance of the first EN
chunk.  The way to determine how a token ID is being used is to check
the next token ID in the sequence.  In rewrite rule 7, 0 is followed
by 24, which is the next token after the EN chunk that starts at token
0, so 0 refers to the entire EN chunk.  For rewrite rule 1, 2 is
followed by 4, which is the next token in the string, so 2 refers to
"2/[NPD".


graph/initial.coref
Columns: caption ID, next token ID, string
The NAME.coref file used to generate the graph, with the token IDs
added to each token in the caption.


graph/np.idx (NP sub-graph)
graph/vp.idx (VP sub-graph)
graph/node.idx (denotation graph)
Columns: node ID, string
A list of node IDs and their associated string.


graph/np-cap.map (NP sub-graph)
graph/vp-cap.map (VP sub-graph)
graph/node-cap.map (denotation graph)
Columns: node ID, caption IDs*
A list of the captions that produce a given node/string.


graph/cap-node.map (denotation graph)
Columns: Caption ID, node IDs*
A list of the nodes/strings that a caption generates.

graph/np-tree.txt (NP sub-graph)
graph/vp-tree.txt (VP sub-graph)
graph/node-tree.txt (denotation graph)
Columns: child (more specific) node ID, edge type,
         parent (more generic) node ID, caption IDs(+ rewrite rule)*
A list of edges in the denotation graph.  The way it should be read
is, child node (more specific, first column) can be produced from
parent node (more generic, third column), by adding an element of the
edge type (second column) to the parent.  The remaining columns are a
list of caption IDs and/or rewrite rules that produced the edge.  The
rewrite rules are of the form "<caption ID>#<rewrite rule #>" - see
"graph/initial.txt" for the list of rewrite rules for each caption.
In cases where only the caption ID is used, the string was rewritten
without using an explicitly generated rewrite rule.  This should
include entity extraction from strings (SENT edges), links between
leaf nodes and the original caption (ORIG edges), and direct object
extractions created during the VP sub-graph generation (some TVERB
edges).

The current types of edges are:
ADVP:          Add an adverbial phrase
RB:            Add an adverb to a verb phrase

NPDET:         Add a noun phrase determiner
NPMOD:         Add a noun phrase modifier
NPHEAD:        Replace the noun phrase head with a more specific term

PP:            Add a prepositional phrase
WEAR:          Add the phrase "wear X"
DRESS:         Add the phrase "dressed in X" or "dressed for X"

Xof:           Add "X of" to a noun phrase, forming "X of <something>"
ofY:           Add "of Y" to a noun phrase, forming "<something> of Y"
Xor:           Add "X or" to a noun phrase, forming "X or <something>"
orY:           Add "or Y" to a noun phrase, forming "<something> or Y"

COMPLEX:       Add the rest of a complex sentence to a simple sentence
COMPLEX-VERB:  Add the rest of a complex sentence to a verb phrase
               (simple sentence with no subject found)

SUBJ:          Add a subject to a verb phrase
VERB:          Add a verb phrase to a subject
DOBJ:          Add a direct object to a transitive verb
TVERB:         Add a transitive verb to a direct object

DROP:          Drop a tailing "while" or "and"

SENT:          Edge from a node consisting of a noun phrase to a node
               which contains the noun phrase, and whose parents do not
ORIG:          Edge from a leaf node to the caption that produced it


graph/np-chunk.txt (NP sub-graph)
graph/vp-chunk.txt (VP sub-graph)
graph/node-chunk.txt (denotation graph)
graph/node-chunk2.txt (combination of the above files)
Columns: node ID, chunked string, caption IDs*
Contains the actual chunking information (i.e., node 0 is "person",
but the chunking is "[EN [NP [NPH person ] ] ]", or "[VP person ]" in
one erroneous case) of the strings.  The remaining columns are a list
of captions that generate that particular chunking.  Empty chunks are
removed, so "[EN [NP [NPH person ] ] ]" covers all of "[EN [NP [NPD ]
[NPH person ] ] ]", "[EN [NP [NPM ] [NPH person ] ] ]", "[EN [NP [NPD
] [NPM ] [NPH person ] ] ]", etc.


graph/token.txt (denotation graph)
Columns: caption ID, tokenized string
Contains the tokenized and lemmatized captions that were used to
generate "graph/initial.txt".


graph/type-chunk.txt (denotation graph)
Columns: node ID, type, chunked string, caption IDs*
Similar to "graph/node-chunk2.txt", except with an added type column.
The type is either EN (entity mention/noun phrase), VP (verb phrase -
may include direct object), or SN (anything else - should be a
sentence).


graph/np-orig.txt (NP sub-graph)
graph/vp-orig.txt (VP sub-graph)
Columns: node ID, caption ID, string
Contains the strings used internally during graph generation process.
These are similar to the starting strings, and have the token IDs
attached.  A line represents a string generated by the given caption,
which has been assigned to the given node.


graph/train/node-image.cnt (denotation graph, training sub-graph only)
Columns: node X, node Y, images producing X, images producing Y, 
         images producing X and Y
Contains the co-occurence counts of X and Y.  Each node is produced by
a set of images (or captions of images, but we back off to images when
we consider visual denotations).  This file contains the number of
images that produced node X, node Y, and that produced both X and Y.
This is equivalent to the size of the visual denotation of X, the
visual denotation of Y, and the intersection of their visual
denotations.

To show up in this file, for a pair of nodes X and Y, both X and Y
must each be produced by at least some number of images (default is
10), and both X and Y must be produced by at least some number of
images (default is 1).


graph/train/node-image.pmi (denotation graph, training sub-graph only)
Columns: pmi(X, Y), P(X | Y), P(Y | X), images producing X, node X,
         images producting Y, node Y
Contains the pointwise mutual information (PMI), and conditional
probabilities for each pair of nodes X and Y in
"graph/train/node-image.cnt".


graph/train/pmi/en-en.pmi (denotation graph, training sub-graph only)
graph/train/pmi/en-vp.pmi
graph/train/pmi/vp-vp.pmi
Columns: pmi(X, Y), P(X | Y), P(Y | X), count of X, typed count of X,
         string X, count of Y, typed count of Y, string Y,
         descendant?, overlap?
Each file contains the PMIs and conditional probabilities, with X and
Y restricted to a particular type (see "graph/type-chunk.txt").  For
"graph/train/pmi/en-vp.pmi" X can always be an entity mention (en),
and Y can always be a verb phrase (vp).  The actual PMIs and
conditional probabilities are the same as those calculated in
"graph/train/node-image.pmi" - we have not actually restricted X or Y
to their particular types.  We have, however included the image count
of X and Y when they are restricted to their assigned type (typed
count).  Finally, the "descendant?" column will have a "D" if X is a
descendant of Y or vice versa, and the "overlap?" column will have an
"O" if X and Y share a token.  In cases where either column has an
entry, it may be the case that the fact that X and Y are related is
uninteresting.

From "graph/train/pmi/vp-vp.pmi":

0.320   1.000   0.005   2192   2   hat   12   12   wear yellow hat    D   O

Since the file is "vp-vp", both "hat" and "wear yellow hat" are being
considred as verb phrases, for the moment.  "hat" is produced by 2192
images, but for only 2 of those images it is considered to be a VP, so
we may not want to consider this entry.  "wear yellow hat" is produced
by 12 images, and in all 12 of those it is considered to be a VP.
Finally, both the "D" and "O" flags exist, so "wear yellow hat" is a
descendant of "hat", and they share at least one token, so this even
if "hat" were a verb phrase, this might still be an uninteresting pair
of strings.
